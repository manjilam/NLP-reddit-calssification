{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42144078-92ab-4fdb-8a56-075ef658ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd157d-6c32-4c95-9e44-9c3e57d5f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit('DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f03c3e-3bbc-4b8f-9587-d96f2c4d79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/75677839/how-to-scrape-all-the-posts-from-a-subreddit-from-a-specific-period-of-time\n",
    "def scrape_subreddit_year_2023(subreddit_name, limit = 1000):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "#duration 2021-2023\n",
    "    start_date = datetime.strptime('01-01-21 00:00:00', '%d-%m-%y %H:%M:%S').timestamp()\n",
    "    end_date = datetime.strptime('31-12-23 23:59:59', '%d-%m-%y %H:%M:%S').timestamp()\n",
    "\n",
    "    \n",
    "    for post in subreddit.top(limit=limit):#gets top post\n",
    "        date = post.created_utc\n",
    "        if start_date <= date <= end_date: #to get posts for the duration\n",
    "            posts.append({\n",
    "                \"title\": post.title,\n",
    "                \"text\": post.selftext,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"created_utc\": post.created_utc\n",
    "            })\n",
    "\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f6c729-b30d-44e7-bbaa-e88c3d9db1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for both 2024 and 2025\n",
    "def scrape_subreddit_year_2025(subreddit_name, limit = 1000): \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "#duration 2024-2025 present\n",
    "    start_date = datetime.strptime('01-01-24 00:00:00', '%d-%m-%y %H:%M:%S').timestamp()\n",
    "    end_date = datetime.strptime('31-03-25 23:59:59', '%d-%m-%y %H:%M:%S').timestamp()\n",
    "\n",
    "    \n",
    "    for post in subreddit.new(limit=limit):#gets new post\n",
    "        date = post.created_utc\n",
    "        if start_date <= date <= end_date: #to get posts for the duration\n",
    "            posts.append({\n",
    "                \"title\": post.title,\n",
    "                \"text\": post.selftext,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"created_utc\": post.created_utc\n",
    "            })\n",
    "\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f90a49a-c459-44bc-9cb1-1c94da503e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_from_declutter_year_2023 = scrape_subreddit_year_2023('Declutter', limit=1000)\n",
    "\n",
    "df = pd.DataFrame(posts_from_declutter_year_2023)\n",
    "df.to_csv('files/declutter_new_year_2023.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8b2e77-b3de-4867-8bce-209b45ccc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_from_minimalism_year_2023 = scrape_subreddit_year_2023('Minimalism', limit=1000)\n",
    "\n",
    "df = pd.DataFrame(posts_from_minimalism_year_2023)\n",
    "df.to_csv('files/minimalism_new_year_2023.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0644d28-4c9d-4e20-871f-f87b9d2b56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_from_declutter_year_2025 = scrape_subreddit_year_2025('Declutter', limit=1000)\n",
    "\n",
    "df = pd.DataFrame(posts_from_declutter_year_2025)\n",
    "df.to_csv('files/declutter_new_year_2024_2025.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ad308f-bab7-4f59-a7a4-a8da86006ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_from_minimalism_year_2025 = scrape_subreddit_year_2025('Minimalism', limit=1000)\n",
    "\n",
    "df = pd.DataFrame(posts_from_minimalism_year_2025)\n",
    "df.to_csv('files/minimalism_new_year_2024_2025.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f54eaf-2a66-481e-93cf-824a631cce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all files are named by their subreddit_redditmethod.csv\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'files')\n",
    "files = [file for file in os.listdir(directory) if file.startswith('declutter') and file.endswith('.csv')]\n",
    "df_list = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "m_declutter_df = pd.concat(df_list, ignore_index=True) # concatanate into one file\n",
    "merged_declutter_df = m_declutter_df.drop_duplicates(keep='first') #remove duplicates posts if overlap\n",
    "sorted_declutter_df = merged_declutter_df.sort_values(by='created_utc', ascending=True) #sort by created_utc \n",
    "data_declutter_df = sorted_declutter_df.head(1000) #get first 1000 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ad1cf3a-caa1-4f6e-a437-b83d5aed80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(os.getcwd(), 'files')\n",
    "files = [file for file in os.listdir(directory) if file.startswith('minimalism') and file.endswith('.csv')]\n",
    "df_list = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "m_minimalism_df = pd.concat(df_list, ignore_index=True)\n",
    "merged_minimalism_df = m_minimalism_df.drop_duplicates(keep='first') #remove duplicates posts if overlap\n",
    "sorted_minimalism_df = merged_minimalism_df.sort_values(by='created_utc', ascending=True) #sort by created_utc \n",
    "data_minimalism_df = sorted_minimalism_df.head(1000) #get first 1000 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a524d7d-54d1-4778-9128-4240f8ccd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([data_declutter_df, data_minimalism_df], ignore_index=True)\n",
    "final_df.to_csv('files/final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54e28d-feed-4caf-86ed-8e8b1ed518e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
